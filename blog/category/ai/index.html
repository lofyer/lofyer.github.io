<!doctype html><html lang=zh class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content=lofyer><link href=https://docs.lofyer.org/blog/category/ai/ rel=canonical><link href=../abm/ rel=prev><link href=../cloud-infra/ rel=next><link rel=icon href=../../../images/favicon.ico><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.34"><title>ai - Lofyer's Blog</title><link rel=stylesheet href=../../../assets/stylesheets/main.35f28582.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#ai class=md-skip> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=页眉> <a href=../../.. title="Lofyer's Blog" class="md-header__button md-logo" aria-label="Lofyer's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Lofyer's Blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> ai </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=搜索 placeholder=搜索 autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=查找> <button type=reset class="md-search__icon md-icon" title=清空当前内容 aria-label=清空当前内容 tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/lofyer/lofyer.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> lofyer/lofyer.github.io </div> </a> </div> </nav> <nav class=md-tabs aria-label=标签 data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Index </a> </li> <li class=md-tabs__item> <a href=../../../wangsheng/ class=md-tabs__link> WangSheng </a> </li> <li class=md-tabs__item> <a href=../../../datanote/ class=md-tabs__link> Archive Docs </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Archive Blog </a> </li> <li class=md-tabs__item> <a href=../../../tags/ class=md-tabs__link> Tags </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=导航栏 data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="Lofyer's Blog" class="md-nav__button md-logo" aria-label="Lofyer's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Lofyer's Blog </label> <div class=md-nav__source> <a href=https://github.com/lofyer/lofyer.github.io title=前往仓库 class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> lofyer/lofyer.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Index </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../wangsheng/ class=md-nav__link> <span class=md-ellipsis> WangSheng </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../datanote/ class=md-nav__link> <span class=md-ellipsis> Archive Docs </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> Archive Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Archive Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ class=md-nav__link> <span class=md-ellipsis> 首页 </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 归档 </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3 checked> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> 分类 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=true> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> 分类 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../lab/ class=md-nav__link> <span class=md-ellipsis> Lab </span> </a> </li> <li class=md-nav__item> <a href=../abm/ class=md-nav__link> <span class=md-ellipsis> abm </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> ai </span> </a> </li> <li class=md-nav__item> <a href=../cloud-infra/ class=md-nav__link> <span class=md-ellipsis> cloud-infra </span> </a> </li> <li class=md-nav__item> <a href=../collect/ class=md-nav__link> <span class=md-ellipsis> collect </span> </a> </li> <li class=md-nav__item> <a href=../devices/ class=md-nav__link> <span class=md-ellipsis> devices </span> </a> </li> <li class=md-nav__item> <a href=../diary/ class=md-nav__link> <span class=md-ellipsis> diary </span> </a> </li> <li class=md-nav__item> <a href=../draft/ class=md-nav__link> <span class=md-ellipsis> draft </span> </a> </li> <li class=md-nav__item> <a href=../filecoin/ class=md-nav__link> <span class=md-ellipsis> filecoin </span> </a> </li> <li class=md-nav__item> <a href=../followmyheart/ class=md-nav__link> <span class=md-ellipsis> followmyheart </span> </a> </li> <li class=md-nav__item> <a href=../fpga/ class=md-nav__link> <span class=md-ellipsis> fpga </span> </a> </li> <li class=md-nav__item> <a href=../hpc/ class=md-nav__link> <span class=md-ellipsis> hpc </span> </a> </li> <li class=md-nav__item> <a href=../inthecloud/ class=md-nav__link> <span class=md-ellipsis> inthecloud </span> </a> </li> <li class=md-nav__item> <a href=../linux-admin/ class=md-nav__link> <span class=md-ellipsis> linux-admin </span> </a> </li> <li class=md-nav__item> <a href=../modeling/ class=md-nav__link> <span class=md-ellipsis> modeling </span> </a> </li> <li class=md-nav__item> <a href=../my-books/ class=md-nav__link> <span class=md-ellipsis> my-books </span> </a> </li> <li class=md-nav__item> <a href=../mylife/ class=md-nav__link> <span class=md-ellipsis> mylife </span> </a> </li> <li class=md-nav__item> <a href=../new-world/ class=md-nav__link> <span class=md-ellipsis> new-world </span> </a> </li> <li class=md-nav__item> <a href=../others/ class=md-nav__link> <span class=md-ellipsis> others </span> </a> </li> <li class=md-nav__item> <a href=../ovs/ class=md-nav__link> <span class=md-ellipsis> ovs </span> </a> </li> <li class=md-nav__item> <a href=../tech/ class=md-nav__link> <span class=md-ellipsis> tech </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label=目录> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=ai>ai</h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-09-22 00:00:00">2019年9月22日</time></li> <li class=md-meta__item> 分类于 <a href=../abm/ class=md-meta__link>abm</a>, <a href=./ class=md-meta__link>ai</a></li> <li class=md-meta__item> 需要 5 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=agent-based-modeling><a href=../../2019/09/22/tensorflow-in-netlogo-make-your-agent-more-intelligent/ class=toclink>Agent based modeling相关</a></h2> <hr> <h3 id=tensorflow-in-netlogo-make-your-agent-more-intelligent><a class=toclink href=../../2019/09/22/tensorflow-in-netlogo-make-your-agent-more-intelligent/#tensorflow-in-netlogo-make-your-agent-more-intelligent>TensorFlow in NetLogo, Make Your Agent More Intelligent</a></h3> <h4 id=0-background><a class=toclink href=../../2019/09/22/tensorflow-in-netlogo-make-your-agent-more-intelligent/#0-background>0. Background</a></h4> <p>NetLogo is a very useful tools for ABM, and Python is also a handful language for building proof of concept.</p> <p>In this post I will show you how to call python language in NetLogo. For more information please <a href=https://github.com/NetLogo/Python-Extension>follow here</a>.</p> <h4 id=1-netlogo-version><a class=toclink href=../../2019/09/22/tensorflow-in-netlogo-make-your-agent-more-intelligent/#1-netlogo-version>1. NetLogo version</a></h4> <p><a href=https://blog.lofyer.org/wp-content/uploads/WX20190922-211123@2x.png><img alt src=/blog/images/WX20190922-211123@2x-1024x958.png></a></p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span>
<span class=normal>16</span>
<span class=normal>17</span>
<span class=normal>18</span>
<span class=normal>19</span>
<span class=normal>20</span>
<span class=normal>21</span>
<span class=normal>22</span>
<span class=normal>23</span>
<span class=normal>24</span>
<span class=normal>25</span>
<span class=normal>26</span>
<span class=normal>27</span>
<span class=normal>28</span>
<span class=normal>29</span>
<span class=normal>30</span>
<span class=normal>31</span>
<span class=normal>32</span>
<span class=normal>33</span>
<span class=normal>34</span>
<span class=normal>35</span>
<span class=normal>36</span>
<span class=normal>37</span>
<span class=normal>38</span>
<span class=normal>39</span>
<span class=normal>40</span>
<span class=normal>41</span>
<span class=normal>42</span>
<span class=normal>43</span>
<span class=normal>44</span>
<span class=normal>45</span>
<span class=normal>46</span>
<span class=normal>47</span>
<span class=normal>48</span>
<span class=normal>49</span>
<span class=normal>50</span>
<span class=normal>51</span>
<span class=normal>52</span>
<span class=normal>53</span>
<span class=normal>54</span>
<span class=normal>55</span>
<span class=normal>56</span>
<span class=normal>57</span>
<span class=normal>58</span>
<span class=normal>59</span>
<span class=normal>60</span>
<span class=normal>61</span>
<span class=normal>62</span>
<span class=normal>63</span>
<span class=normal>64</span>
<span class=normal>65</span>
<span class=normal>66</span>
<span class=normal>67</span>
<span class=normal>68</span>
<span class=normal>69</span>
<span class=normal>70</span>
<span class=normal>71</span>
<span class=normal>72</span>
<span class=normal>73</span>
<span class=normal>74</span>
<span class=normal>75</span>
<span class=normal>76</span>
<span class=normal>77</span>
<span class=normal>78</span>
<span class=normal>79</span>
<span class=normal>80</span>
<span class=normal>81</span>
<span class=normal>82</span>
<span class=normal>83</span></pre></div></td><td class=code><div><pre><span></span><code>breed [data-points data-point]
breed [centroids centroid]

globals [
  any-centroids-moved?
]

to setup
  clear-all
  set-default-shape data-points &quot;circle&quot;
  set-default-shape centroids &quot;x&quot;
  generate-clusters
  reset-centroids
end

to generate-clusters
  let cluster-std-dev 20 - num-clusters
  let cluster-size num-data-points / num-clusters
  repeat num-clusters [
    let center-x random-xcor / 1.5
    let center-y random-ycor / 1.5
    create-data-points cluster-size [
      setxy center-x center-y
      set heading random 360
      fd abs random-normal 0 (cluster-std-dev / 2) ;; Divide by two because abs doubles the width
    ]
  ]
end

to reset-centroids
  set any-centroids-moved? true
  ask data-points [ set color grey ]

  let colors base-colors
  ask centroids [die]
  create-centroids num-centroids [
    move-to one-of data-points
    set size 5
    set color last colors + 1
    set colors butlast colors
  ]
  clear-all-plots
  reset-ticks
end

to go
  if not any-centroids-moved? [stop]
  set any-centroids-moved? false
  assign-clusters
  update-clusters
  tick
end

to assign-clusters
  ask data-points [set color [color] of closest-centroid - 2]
end

to update-clusters
  let movement-threshold 0.1
  ask centroids [
    let my-points data-points with [ shade-of? color [ color ] of myself ]
    if any? my-points [
      let new-xcor mean [ xcor ] of my-points
      let new-ycor mean [ ycor ] of my-points
      if distancexy new-xcor new-ycor &gt; movement-threshold [
        set any-centroids-moved? true
      ]
      setxy new-xcor new-ycor
    ]
  ]
  update-plots
end

to-report closest-centroid
  report min-one-of centroids [ distance myself ]
end

to-report square-deviation
  report sum [ (distance myself) ^ 2 ] of data-points with [ closest-centroid = myself ]
end

; Copyright 2014 Uri Wilensky.
; See Info tab for full copyright and license.
</code></pre></div></td></tr></table></div> <h4 id=2-tensorflow-version><a class=toclink href=../../2019/09/22/tensorflow-in-netlogo-make-your-agent-more-intelligent/#2-tensorflow-version>2. TensorFlow version</a></h4> <p>TensorFlow version: 1.14</p> <div class=highlight><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal> 1</span>
<span class=normal> 2</span>
<span class=normal> 3</span>
<span class=normal> 4</span>
<span class=normal> 5</span>
<span class=normal> 6</span>
<span class=normal> 7</span>
<span class=normal> 8</span>
<span class=normal> 9</span>
<span class=normal>10</span>
<span class=normal>11</span>
<span class=normal>12</span>
<span class=normal>13</span>
<span class=normal>14</span>
<span class=normal>15</span>
<span class=normal>16</span>
<span class=normal>17</span>
<span class=normal>18</span>
<span class=normal>19</span>
<span class=normal>20</span>
<span class=normal>21</span>
<span class=normal>22</span>
<span class=normal>23</span>
<span class=normal>24</span>
<span class=normal>25</span>
<span class=normal>26</span>
<span class=normal>27</span>
<span class=normal>28</span>
<span class=normal>29</span>
<span class=normal>30</span>
<span class=normal>31</span>
<span class=normal>32</span>
<span class=normal>33</span></pre></div></td><td class=code><div><pre><span></span><code>import numpy as np
import tensorflow as tf

num-points = 100
dimensions = 2
points = np.random.uniform(0, 1000, [num-points, dimensions])

def input-fn():
  return tf.compat.v1.train.limit-epochs(
      tf.convert-to-tensor(points, dtype=tf.float32), num-epochs=1)

num-clusters = 5
kmeans = tf.contrib.factorization.KMeansClustering(
    num-clusters=num-clusters, use-mini-batch=False)

# train
num-iterations = 10
previous-centers = None
for - in xrange(num-iterations):
  kmeans.train(input-fn)
  cluster-centers = kmeans.cluster-centers()
  if previous-centers is not None:
    print &#39;delta:&#39;, cluster-centers - previous-centers
  previous-centers = cluster-centers
  print &#39;score:&#39;, kmeans.score(input-fn)
print &#39;cluster centers:&#39;, cluster-centers

# map the input points to their clusters
cluster-indices = list(kmeans.predict-cluster-index(input-fn))
for i, point in enumerate(points):
  cluster-index = cluster-indices[i]
  center = cluster-centers[cluster-index]
  print &#39;point:&#39;, point, &#39;is in cluster&#39;, cluster-index, &#39;centered at&#39;, center
</code></pre></div></td></tr></table></div> <h4 id=3-netlogo-with-python-extension-version><a class=toclink href=../../2019/09/22/tensorflow-in-netlogo-make-your-agent-more-intelligent/#3-netlogo-with-python-extension-version>3. NetLogo with Python Extension version</a></h4> <p>Here's the snapshot.</p> <p><a href=https://blog.lofyer.org/wp-content/uploads/WX20191028-204937@2x.png><img alt src=/blog/images/WX20191028-204937@2x.png></a></p> <p>And here's the code.</p> <p>extensions [ py ]</p> <p>breed [data-points data-point] breed [centroids centroid]</p> <p>data-points-own [ cluster-id ]</p> <p>centroids-own [ cluster-id centx centy ]</p> <p>globals [ testoutput centroid-list ]</p> <p>to setup clear-all py:setup py:python (py:run "import tensorflow as tf" "import numpy as np" ) set testoutput py:runresult "1" py:set "testoutput" testoutput set-default-shape data-points "circle" set-default-shape centroids "x" generate-clusters ; For python py:set "num-points" num-clusters py:set "points" [list xcor ycor] of data-points py:set "num-clusters" num-clusters py:set "num-round" num-round if debug = True [ py:run "print('Points Cordinates:', points)" ;for debug ] ;reset-centroids end</p> <p>to generate-clusters set testoutput py:runresult "testoutput + 1" let cluster-std-dev cluster-range let cluster-size num-data-points / num-clusters repeat num-clusters [ let center-x random-xcor / 1.5 let center-y random-ycor / 1.5 create-data-points cluster-size [ setxy center-x center-y set heading random 360 fd abs random-normal 0 (cluster-std-dev / 2) ] ] end</p> <p>to train ; Cluster center (py:run "points = np.asarray(points)" "def input-fn():" " return tf.compat.v1.train.limit-epochs(tf.convert-to-tensor(points, dtype=tf.float32), num-epochs=1)" "kmeans = tf.contrib.factorization.KMeansClustering(num-clusters=num-clusters, use-mini-batch=False)" "num-iterations = num-round" "previous-centers = None" "for - in range(num-iterations):" " kmeans.train(input-fn)" " cluster-centers = kmeans.cluster-centers()" " if previous-centers is not None:" " print(('delta:', cluster-centers - previous-centers))" " previous-centers = cluster-centers" " print(('score:', kmeans.score(input-fn)))" "print(('cluster centers:', cluster-centers))" "# map the input points to their clusters" "cluster-indices = list(kmeans.predict-cluster-index(input-fn))" "print('cluster indices: ', cluster-indices)" "for i, point in enumerate(points):" " cluster-index = cluster-indices[i]" " center = cluster-centers[cluster-index]" " print(('point:', point, 'is in cluster', cluster-index, 'centered at', center))" ) end</p> <p>to show-shape set centroid-list py:runresult "cluster-centers" foreach centroid-list [ x -&gt; create-centroids 1 [ set xcor ( item 0 x ) set ycor ( item 1 x ) set size 3 set color white ] ] end</p> <p>Ref:</p> <p>[1] <a href=https://www.altoros.com/blog/using-k-means-clustering-in-tensorflow/ >https://www.altoros.com/blog/using-k-means-clustering-in-tensorflow/</a></p> <p>[2] <a href=https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering>https://www.tensorflow.org/api-docs/python/tf/contrib/factorization/KMeansClustering</a></p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2019-04-08 00:00:00">2019年4月8日</time></li> <li class=md-meta__item> 分类于 <a href=./ class=md-meta__link>ai</a>, <a href=../devices/ class=md-meta__link>devices</a></li> <li class=md-meta__item> 需要 7 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=nvidia-jetson><a href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/ class=toclink>NVIDIA Jetson使用指导</a></h2> <p>本文将以NVIDIA Jetson为硬件基础，为你展现NVIDIA的力量，可以将其作为Jetson Nano的入门参考手册（教程）。</p> <h3 id=1><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#1>1. 入门篇</a></h3> <p>入门篇的有两章内容，来自NVIDIA JETSON包装盒自带的内容。</p> <h4 id=11><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#11>1.1. 准备环境</a></h4> <p>Ref.1: <a href=https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit>https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit</a></p> <p>Ref.2: <a href=https://developer.nvidia.com/embedded/downloads>https://developer.nvidia.com/embedded/downloads</a></p> <p>这里我使用的是一个Jetson Nano开发板，起初我以为随便一个USB数据线就能把它跑起来，但是我想多了。18W的快充头插上后可以正常启动，但是一旦运行比如WebGL测试的页面直接就关机了，因而我把小米音箱的电源适配器（5V2A）给它用了，还真能跑的动。</p> <p>但是为了接下来的内容，我特意买了一个5V4A的5.5mm OD的电源适配器，要不然以我现在的环境很难保证能过了接下来的准备环境阶段。</p> <p>要使用这个板子，你需要提前下载<a href=https://developer.nvidia.com/embedded/dlc/jetson-nano-dev-kit-sd-card-image>SDK的SD卡镜像</a>，以及Host OS（PC，Ubuntu 18.04）所需的<a href=https://developer.nvidia.com/embedded/dlc/nv-sdk-manager>SDK Manager</a>。</p> <p>下载完成之后，从Ref.1的链接中下载所需的镜像烧录软件或者别的烧录软件也行，将SD卡镜像烧录至TF卡中（这里我使用的是64G TF卡，A2）。烧录完毕将其插入核心板下面的卡槽中（有些不好找），见下图。</p> <p><a href=https://blog.lofyer.org/wp-content/uploads/微信图片_20190411181357.jpg><img alt src=/blog/images/微信图片_20190411181357.jpg></a></p> <p>然后通过HDMI/DP线将之与显示器连接（启动阶段的分辨率需要修改，要不然小点的屏幕没法显示启动logo，这个不是重点以后再说），插上蓝牙USB键鼠，接入USB电源，你就可以看到信仰之NVIDIA logo。</p> <p><a href=https://blog.lofyer.org/wp-content/uploads/微信图片_20190411181347.jpg><img alt src=/blog/images/微信图片_20190411181347.jpg></a></p> <p>这里你需要等它初始化完成，初始化工作包括扩展跟文件系统、解压乱七八糟的包之类的，总之等看到Ubuntu Desktop的安装配置界面后可以开始操作了。</p> <p>进入桌面后的第一件事儿，可以先打开Chromium，访问<a href=https://webglsamples.org/ >WebGL的示例网站</a>，随便开个demo试试会不会关机，如果关机那么恭喜你可以找个正经的USB电源了（Ref.1里有Adafruit的USB和DC电源适配器购买链接）。</p> <p>当你的DC电源到了以后，先不要直接插上，因为需要设置一下跳线，如图所示。</p> <p><a href=https://blog.lofyer.org/wp-content/uploads/f449ce42a97d019b5225c7c220e72ba7.png><img alt src=/blog/images/f449ce42a97d019b5225c7c220e72ba7.png></a></p> <p>看到Power Jack/USB Jumper没，由于我手里没有跳线帽，所以我直接短接了它们，如图所示（看我意念焊接术）。</p> <p><a href=https://blog.lofyer.org/wp-content/uploads/微信图片编辑_20190411181008.jpg><img alt src=/blog/images/微信图片编辑_20190411181008.jpg></a></p> <p>然后再插上刚入手的DC 5V4A，即可空出你的USB并将之与Host PC相连了。</p> <h4 id=12-sdk><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#12-sdk>1.2. 准备SDK</a></h4> <p><strong>这一节你可以先跳过去，等跑完下面的小节后再看，因为这部分并不影响接下来的操作。</strong></p> <p>准备SDK的内容主要包括：下载安装Host PC、开发板所需CUDA、OpenCV之类的，需要开发板的USB连接到Host PC上作数据连接用（我没有尝试过那个USB口既作电源又作数据传输）。</p> <p><a href=https://blog.lofyer.org/wp-content/uploads/捕获-1.png><img alt src=/blog/images/捕获-1.png></a></p> <p>这里我并不打算过多介绍，只要按照引导进行操作即可。</p> <h4 id=13-hello-ai-world><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#13-hello-ai-world>1.3. Hello AI World</a></h4> <p>这个示例为你充分避开了各种依赖库的复杂安装步骤以及非常多的专业术语，对新手较为友好，但是我会仍会将其以链接形式展现，在最后章节的连接中。</p> <p>第一步，从Hello World开始，你仍然需要最基础的工具。</p> <p>$ sudo apt install git make cmake $ git clone https://github.com/dusty-nv/jetson-inference $ cd jetson-inference $ git submodule update --init $ mkdir build $ cd build $ cmake ../ $ make -j4 $ sudo make install $ cd aarch64/bin</p> <p>这一顿操作后，你会拥有个Hello AI World的全部成果。但是，what the hell an I doing?</p> <p>来，对于一些Linux不熟悉的同学来说只要知道这里的cmake与make是编译源码的指令就行，cmake用来生成Makefile，make会根据Makefile里定义的动作调用gcc开始编译。</p> <p>然后让我们看第一个例子，使用<a href=http://www.image-net.org/ >ImageNet</a>的图片素材来训练我们的“机器人”让它能够识别各种物体，其中你会看到当前目录下有两个imagenet开头的文件，让我们从imagenet-console开始。</p> <p>在图形界面上打开终端后，执行如下命令。</p> <p>$ ./imagenet-console orange_0.jpg output_0.jpg $ nautilus .</p> <p>然后经过机器人的推理以后，你会得到一张橘子、另一张还是橘子的图片，并且新橘子图片的左上角标识了机器人认为它有多大概率是橘子。</p> <p><img alt src=/blog/images/捕获.png></p> <p>是不是有感觉了？OK，我们继续。</p> <p>既然它可以看图片，那么它当然也可以看视频或者摄像头中的内容，那么接下来我们让它看到摄像头中的橘子试试。</p> <p>这里我需要给Jetson Nano接一个USB摄像头，接入以后可以在终端键入cheese打开拍照应用程序看它是否工作。</p> <p>然后终端中运行如下命令。</p> <p>./imagenet-camera</p> <p>Oops，segmentation fault了，<a href=https://github.com/dusty-nv/jetson-inference/blob/master/docs/imagenet-camera-2.md>如文档所说</a>，默认的摄像头是板载CSI摄像头，所以这里需要修改代码让它使用后来插入的USB摄像头。</p> <p>$ vi ../../../imagenet-camera/imagenet-camera.cpp</p> <p>...</p> <h2 id=include-imageneth><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#include-imageneth>include "imageNet.h"</a></h2> <h2 id=define-default_camera-0-1-for-onboard-camera-or-change-to-index-of-devvideo-v4l2-camera-0><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#define-default_camera-0-1-for-onboard-camera-or-change-to-index-of-devvideo-v4l2-camera-0>define DEFAULT_CAMERA 0 // -1 for onboard camera, or change to index of /dev/video V4L2 camera (&gt;=0)</a></h2> <p>bool signal_recieved = false; ...</p> <p>将DEFAULT_CAMERA修改为0以后，便会启用/dev/video0路径上的摄像头，然后重新编译。</p> <p>$ cd ../../ # build $ cmake ../ $ make -j4</p> <p>然后进到bin目录下再运行一次imagenet-camera即可。</p> <p><strong>PS：不是所有的摄像头都叫罗技C920，由于摄像头原生编码的问题，可能会导致上述程序黑屏，那么我们需要是适当修改一些内容。笔者暂时跳过这里，等改好以后再看。关于兼容列表可以参考<a href=https://elinux.org/Jetson_Nano#Ecosystem_Products_and_Sensors>eLinux的链接</a>。</strong></p> <h4 id=14><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#14>1.4. 写一个小程序</a></h4> <p>如果你跟着github的教程，那么应该到你自己写一段代码的时间了，直接粘贴吧。</p> <p>// include imageNet header for image recognition</p> <h2 id=include><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#include>include <jetson-inference imagenet.h></a></h2> <p>// // include loadImage header for loading images</p> <h2 id=include_1><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#include_1>include <jetson-utils loadimage.h></a></h2> <p>int main( int argc, char** argv ) { // a command line argument containing the image filename is expected, // // so make sure we have at least 2 args (the first arg is the program) if( argc &lt; 2 ) { printf("my-recognition: expected image filename as argument\n"); printf("example usage: ./my-recognition my_image.jpg\n"); return 0; }</p> <p>// retrieve the image filename from the array of command line args const char* imgFilename = argv[1]; float* imgCPU = NULL; // CPU pointer to floating-point RGBA image data float* imgCUDA = NULL; // GPU pointer to floating-point RGBA image data int imgWidth = 0; // width of the image (in pixels) int imgHeight = 0; // height of the image (in pixels)</p> <p>// load the image from disk as float4 RGBA (32 bits per channel, 128 bits per pixel) if( !loadImageRGBA(imgFilename, (float4**)&amp;imgCPU, (float4**)&amp;imgCUDA, &amp;imgWidth, &amp;imgHeight) ) { printf("failed to load image '%s'\n", imgFilename); return 0; } imageNet* net = imageNet::Create(imageNet::GOOGLENET);</p> <p>if( !net ) { printf("failed to load image recognition network\n"); return 0; } float confidence = 0.0; const int classIndex = net-&gt;Classify(imgCUDA, imgWidth, imgHeight, &amp;confidence); if( classIndex &gt;= 0 ) { // retrieve the name/description of the object class index const char* classDescription = net-&gt;GetClassDesc(classIndex); // print out the classification results printf("image is recognized as '%s' (class #%i) with %f%% confidence\n", classDescription, classIndex, confidence * 100.0f); } else { // if Classify() returned &lt; 0, an error occurred printf("failed to classify image\n"); } delete net; // this is the end of the example! return 0; }</p> <h2 id=require-cmake-28-or-greater><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#require-cmake-28-or-greater>require CMake 2.8 or greater</a></h2> <p>cmake_minimum_required(VERSION 2.8)</p> <h2 id=declare-my-recognition-project><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#declare-my-recognition-project>declare my-recognition project</a></h2> <p>project(my-recognition)</p> <h2 id=import-jetson-inference-and-jetson-utils-packages><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#import-jetson-inference-and-jetson-utils-packages>import jetson-inference and jetson-utils packages.</a></h2> <h2 id=note-that-if-you-didnt-do-sudo-make-install><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#note-that-if-you-didnt-do-sudo-make-install>note that if you didn't do "sudo make install"</a></h2> <h2 id=while-building-jetson-inference-this-will-error><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#while-building-jetson-inference-this-will-error>while building jetson-inference, this will error.</a></h2> <p>find_package(jetson-utils) find_package(jetson-inference)</p> <h2 id=cuda-and-qt4-are-required><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#cuda-and-qt4-are-required>CUDA and Qt4 are required</a></h2> <p>find_package(CUDA) find_package(Qt4)</p> <h2 id=setup-qt4-for-build><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#setup-qt4-for-build>setup Qt4 for build</a></h2> <p>include(<span qt_95_definitions=QT_DEFINITIONS class=arithmatex>\({QT\_USE\_FILE}) add\_definitions(\)</span>)</p> <h2 id=compile-the-my-recognition-program><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#compile-the-my-recognition-program>compile the my-recognition program</a></h2> <p>cuda_add_executable(my-recognition my-recognition.cpp)</p> <h2 id=link-my-recognition-to-jetson-inference-library><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#link-my-recognition-to-jetson-inference-library>link my-recognition to jetson-inference library</a></h2> <p>target_link_libraries(my-recognition jetson-inference)</p> <p>然后编译并运行。</p> <p>$ cmake . $ make $ ./my.cpp polarbear.jpg</p> <p>输出结果如下。</p> <p>[cuda] cudaAllocMapped 5089520 bytes, CPU 0x100c30000 GPU 0x100c30000</p> <p>imageNet -- loading classification network model from: -- prototxt networks/googlenet.prototxt -- model networks/bvlc_googlenet.caffemodel -- class_labels networks/ilsvrc12_synset_words.txt -- input_blob 'data' -- output_blob 'prob' -- batch_size 2</p> <p>[TRT] TensorRT version 5.0.6 [TRT] detected model format - caffe (extension '.caffemodel') [TRT] desired precision specified for GPU: FASTEST [TRT] requested fasted precision for device GPU without providing valid calibrator, disabling INT8 [TRT] native precisions detected for GPU: FP32, FP16 [TRT] selecting fastest native precision for GPU: FP16 [TRT] attempting to open engine cache file /usr/local/bin/networks/bvlc_googlenet.caffemodel.2.1.GPU.FP16.engine [TRT] loading network profile from engine cache... /usr/local/bin/networks/bvlc_googlenet.caffemodel.2.1.GPU.FP16.engine [TRT] device GPU, /usr/local/bin/networks/bvlc_googlenet.caffemodel loaded [TRT] device GPU, CUDA engine context initialized with 2 bindings [TRT] binding -- index 0 -- name 'data' -- type FP32 -- in/out INPUT -- # dims 3 -- dim #0 3 (CHANNEL) -- dim #1 224 (SPATIAL) -- dim #2 224 (SPATIAL) [TRT] binding -- index 1 -- name 'prob' -- type FP32 -- in/out OUTPUT -- # dims 3 -- dim #0 1000 (CHANNEL) -- dim #1 1 (SPATIAL) -- dim #2 1 (SPATIAL) [TRT] binding to input 0 data binding index: 0 [TRT] binding to input 0 data dims (b=2 c=3 h=224 w=224) size=1204224 [cuda] cudaAllocMapped 1204224 bytes, CPU 0x101310000 GPU 0x101310000 [TRT] binding to output 0 prob binding index: 1 [TRT] binding to output 0 prob dims (b=2 c=1000 h=1 w=1) size=8000 [cuda] cudaAllocMapped 8000 bytes, CPU 0x101440000 GPU 0x101440000 device GPU, /usr/local/bin/networks/bvlc_googlenet.caffemodel initialized. [TRT] networks/bvlc_googlenet.caffemodel loaded imageNet -- loaded 1000 class info entries networks/bvlc_googlenet.caffemodel initialized. class 0296 - 1.000000 (ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus) image is recognized as 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus' (class #296) with 100.000000% confidence</p> <p>简言之，程序使用ImageNet的图片配上GoogleNet的模型对你的图片进行推理，然后得出它认为这是北极熊的可能性。</p> <p>因为这篇文章的目的是入门，也就是带进来以后看哪个方向就自己看，所以相关数学知识在这里已经忽略了，如果你能写出厉害的理论paper又做出很厉害的工程实现，那么大牛就请继续往下过，顺便留个言交个朋友让我膜拜一下。你也可以查看文末的链接进一步扩展阅读什么是GoogleNet，什么是CNN，然后撸一遍机器学习、深度学习、强化学习啥的，也可能一路懵懂复习到信号与系统、高等数学，你要书的话我这还卖，另外我的学习笔记可以参阅<a href=https://datanote.readthedocs.io/zh/master/ >DataNote</a>。</p> <h3 id=2><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#2>2. 进阶篇</a></h3> <h3 id=21><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#21>2.1. 重新训练模型</a></h3> <h3 id=22><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#22>2.2. 作为推理节点</a></h3> <h3 id=23><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#23>2.3. 深度学习实验</a></h3> <h3 id=24-tensorflow><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#24-tensorflow>2.4. TensorFlow实验</a></h3> <p>安装TensorFlow</p> <p>$ sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev $ sudo apt-get install python3-pip $ sudo pip3 install -U pip $ sudo pip3 install -U numpy grpcio absl-py py-cpuinfo psutil portpicker six mock requests gast h5py astor termcolor protobuf keras-applications keras-preprocessing wrapt google-pasta $ sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v42 tensorflow-gpu==1.14.0+nv19.9</p> <p>https://devtalk.nvidia.com/default/topic/1048776/official-tensorflow-for-jetson-nano-/</p> <p>https://www.tensorflow.org/tutorials</p> <p>with jupyter</p> <h4 id=25-tensorrt><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#25-tensorrt>2.5. TensorRT实验</a></h4> <h3 id=3><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#3>3. 行业方案篇</a></h3> <p>太阳底下没有新东西，我发现把之前的笔记稍微整理点可以新开一个目录出一个系列，那么，这里我就直接写关键字吧，以后说不定又冒出什么新东西了呢。</p> <p>在继续之前，我们需要抓住一样内容，即凡是人类自己通过观察、模仿、学习可以获得的重复能力，机器/深度/强化等内容理论来说都可以帮你实现，即使是创造力。</p> <p>然后我们再讲方案，即在这个小小的盒子，它有多少能力，能干啥。</p> <h3 id=31><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#31>3.1. 边缘计算</a></h3> <p>工业现场</p> <p>计算转移</p> <p>CDN</p> <p>军用头戴</p> <h3 id=32><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#32>3.2. 云游戏</a></h3> <p>4K/8K/HDR/60FPS家用主机、服务端</p> <p>无主机头戴</p> <h3 id=33><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#33>3.3. 教学设备</a></h3> <p>甭管便宜的贵的只要带卡都能当教学设备，不信你看研究生论文有多少CUDA相关。</p> <h3 id=34><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#34>3.4. 残障辅助</a></h3> <p>手语翻译（<a href=https://github.com/EvilPort2/Sign-Language>https://github.com/EvilPort2/Sign-Language</a>），可按照中国残联手语进行训练（<a href=http://www.cdpf.org.cn/special/zgsy/node_305701.htm>http://www.cdpf.org.cn/special/zgsy/node_305701.htm</a>）。</p> <p>apt install python3-keras</p> <p>辅助视觉（物体/人脸识别后转语音）</p> <h2 id=_1><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#_1>参考：</a></h2> <h3 id=_2><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#_2>文献</a></h3> <p>[1] NVIDIA AI Two Days Demo: <a href=https://developer.nvidia.com/embedded/twodaystoademo>https://developer.nvidia.com/embedded/twodaystoademo</a></p> <p>[2] CNN Architecture: <a href=https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5>https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5</a></p> <p>[3] eLinux: <a href=https://www.elinux.org/Jetson>https://www.elinux.org/Jetson</a></p> <h3 id=_3><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#_3>术语</a></h3> <p>[1] ImageNet: <a href=http://image-net.org/download>http://image-net.org/download</a></p> <p>[2] TensorRT: <a href=https://developer.nvidia.com/tensorrt>https://developer.nvidia.com/tensorrt</a></p> <p>[3] DeepStream: <a href=https://developer.nvidia.com/deepstream-sdk>https://developer.nvidia.com/deepstream-sdk</a></p> <p>[4] cuDNN</p> <p>[5] PyTorch</p> <h3 id=6-nvdla><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#6-nvdla>[6] NVDLA</a></h3> <p>title: "向NVIDIA Jetson Nano中移植QEMU-KVM" date: 2019-04-14 categories: - "cloud-infra" - "devices"</p> <hr> <p>因为Jetson如果作为边缘设备，那么我们需要进一步探索虚拟化在其上的可能性，从而使FT有更容易的路线可走，还有既然它的芯片是PCIE的，那理应可以透传。</p> <p>参考：<a href=https://elinux.org/Jetson/Nano/Upstream>https://elinux.org/Jetson/Nano/Upstream</a></p> <p>什么build rootfs、uboot之类的就不要了，那是后期嵌入式的活，我们在现有环境上build kernel即可。</p> <h2 id=1_1><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#1_1>1. 准备环境</a></h2> <p>访问链接<a href=https://developer.nvidia.com/embedded/downloads>https://developer.nvidia.com/embedded/downloads</a>并下载源码包，包括Jetson自有以及L4T源码，也可以点击如下链接直接下载。</p> <p>https://developer.nvidia.com/embedded/dlc/l4t-sources-32-1-jetson-nano</p> <p>解压其中的kernel部分。</p> <p>https://developer.nvidia.com/embedded/dlc/l4t-jetson-driver-package-32-1-jetson-nano</p> <p>下载并解压后，得到如下文件系统。</p> <h2 id=2-kernel><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#2-kernel>2. 准备kernel</a></h2> <p>Host: sudo su sudo apt install nfs-kernel-server sudo echo "/home/lofyer/Downloads *(rw,no_root_squash,no_subtree_check)" &gt;&gt; /etc/exports sudo exportfs -avf</p> <p>Jetson Nano: sudo su apt instlal libncurses-dev mount root@192.168.0.59:/home/lofyer/Downloads /mnt cd /mnt/ cp /proc/config.gz . gunzip config.gz mv config .config make menuconfig # find and enable kvm, tegra hypervisor make -j4; make -j4 modules_install make -j4 Image cp arch/arm64/boot/Image /boot/Image-kvm</p> <p>然后编辑启动项，默认从新kernel启动。</p> <h2 id=vi-bootextlinuxextlinuxconf><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#vi-bootextlinuxextlinuxconf>vi /boot/extlinux/extlinux.conf</a></h2> <p>TIMEOUT 10 DEFAULT secondary</p> <p>MENU TITLE p3450-porg eMMC boot options</p> <p>LABEL primary MENU LABEL primary kernel LINUX /boot/Image INITRD /boot/initrd APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk0p1 rw rootwait</p> <p>LABEL secondary MENU LABEL kernel with kvm LINUX /boot/Image-kvm INITRD /boot/initrd APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk0p1 rw rootwait</p> <h2 id=3-qemu-kvm><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#3-qemu-kvm>3. 尝试qemu-kvm</a></h2> <p>自带的：</p> <p>apt install qemu-kvm kvm --help</p> <p>自己编的：</p> <p>git clone https://github.com/qemu/qemu cd qemu ./configure --enable-kvm make -j4</p> <p>只能使用machine类型为arm进行加速。</p> <h2 id=4-ft><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#4-ft>4. 看看FT</a></h2> <h3 id=_4><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#_4>算了，现在不看了，等下半年。</a></h3> <p>title: "基于ARM(NVIDIA-JETSON-NANO)编译NetLogo" date: 2019-04-06 categories: - "abm" - "devices" -tags: - "NVIDIA" - "Jetson Nano" - "NetLogo"</p> <hr> <p>本文主要目的为测试这块板子的性能，看看其是否有作为边缘节点的能力。</p> <h3 id=env><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#env>Env</a></h3> <p>Ubuntu 18.04(jetson-nano-sd-r32.1-2019-03-18)</p> <h3 id=prepare><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#prepare>Prepare</a></h3> <p>$ sudo set -i 's/ports.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list $ sudo apt install -y curl x11vnc openjdk openjfx libopenjfx-jni libopenjfx-java</p> <h3 id=build><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#build>Build</a></h3> <p>$ git clone https://github.com/NetLogo/NetLogo $ cd NetLogo $ git submodule update --init $ ./sbt netlogo/compile</p> <p>Run and Package</p> <p>$ ./sbt netlogo/run $ ./sbt dist/buildNetLogo</p> <h3 id=addon-add-vnc-server-to-your-board><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#addon-add-vnc-server-to-your-board>Addon: Add VNC server to your board</a></h3> <p>$ sudo apt install -y x11nvc $ x11vnc # to generate ~/.vnc files $ echo 'x11vnc --loop &amp;' &gt;&gt; ~/.xsessionrc</p> <h3 id=ref><a class=toclink href=../../2019/04/08/nvidia-jetson%E4%BD%BF%E7%94%A8%E6%8C%87%E5%AF%BC/#ref>Ref</a></h3> <p><a href=https://github.com/NetLogo/NetLogo/wiki/Building>https://github.com/NetLogo/NetLogo/wiki/Building</a></p> <p><a href=https://github.com/NetLogo/NetLogo/wiki/Releasing>https://github.com/NetLogo/NetLogo/wiki/Releasing</a></p> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2013-04-15 00:00:00">2013年4月15日</time></li> <li class=md-meta__item> 分类于 <a href=./ class=md-meta__link>ai</a>, <a href=../linux-admin/ class=md-meta__link>linux-admin</a>, <a href=../others/ class=md-meta__link>others</a></li> <li class=md-meta__item> 需要 1 分钟阅读时间 </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=rex><a href=../../2013/04/15/rex%E8%AE%BE%E8%AE%A1/ class=toclink>Rex设计</a></h2> <p><a href=https://blog.loyfer.org/wp-content/uploads/2013/04/rex-11.png><img alt=rex-1 src=/blog/images/rex-11.png></a></p> <p>目前已完成 capture，dlayer和tlayer。 IplImage 和 Mat转换可考虑不用了 在写交互的部分，后期完成可能会用gtk。 需要的传感器： 加速度（FPV向） ROI：移动检测 抛弃gtk（时间长，没意思），转用ncurses</p> </div> </article> <nav class=md-pagination> </nav> </div> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2008 - 2024 lofyer </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["search.suggest", "search.highlight", "navigation.tabs", "navigation.tabs.sticky", "navigation.tracking", "navigation.prune", "navigation.instant", "navigation.instant.progess"], "search": "../../../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../../../assets/javascripts/bundle.56dfad97.min.js></script> <script src=../../../javascripts/config.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>